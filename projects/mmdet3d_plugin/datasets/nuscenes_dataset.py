# Copyright (c) Wang, Z
# ------------------------------------------------------------------------
# Modified from StreamPETR (https://github.com/exiawsh/StreamPETR)
# Copyright (c) Shihao Wang
# ------------------------------------------------------------------------
# Copyright (c) 2022 megvii-model. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from DETR3D (https://github.com/WangYueFt/detr3d)
# Copyright (c) 2021 Wang, Yue
# ------------------------------------------------------------------------
# Modified from mmdetection3d (https://github.com/open-mmlab/mmdetection3d)
# Copyright (c) OpenMMLab. All rights reserved.
# ------------------------------------------------------------------------

import os.path as osp
import torch
import numpy as np
import random
import math
import mmcv
from mmdet3d.datasets import NuScenesDataset
from mmdet.datasets import DATASETS
from nuscenes.eval.common.utils import Quaternion
from mmcv.parallel import DataContainer as DC
from mmdet.datasets.api_wrappers import COCO
from mmcv.ops.nms import batched_nms
from nuscenes.utils.data_classes import Box
from pyquaternion import Quaternion
from mmdet3d.core.bbox import Box3DMode, points_cam2img, LiDARInstance3DBoxes
from mmdet.core.bbox.iou_calculators import bbox_overlaps

@DATASETS.register_module()
class CustomNuScenesDataset(NuScenesDataset):
    r"""NuScenes Dataset.

    This datset only add camera intrinsics and extrinsics to the results.
    """

    def __init__(self, collect_keys, seq_mode=False, seq_split_num=1, num_frame_losses=1, queue_length=8, random_length=0,
                 seq_length=-1, proposal_file=None, ann_2d_file=None, proposal_nms=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.queue_length = queue_length
        self.collect_keys = collect_keys
        self.random_length = random_length
        self.num_frame_losses = num_frame_losses
        self.seq_mode = seq_mode
        if seq_mode:
            self.num_frame_losses = 1
            self.queue_length = 1
            self.seq_split_num = seq_split_num
            self.seq_length = seq_length
            self.random_length = 0
            self._set_sequence_group_flag() # Must be called after load_annotations b/c load_annotations does sorting.

        self.proposal_file = proposal_file
        self.ann_2d_file = ann_2d_file
        self.proposal_nms = proposal_nms
        self.load_proposals()

    def load_proposals(self):
        if self.proposal_file is None:
            return

        if self.ann_2d_file is None:
            self.proposal_coco = COCO(self.proposal_file)
        else:
            class COCO_wrapper(COCO):
                def __init__(self, coco_file):
                    for k, v in vars(coco_file).items():
                        setattr(self, k, v)
                    self.img_ann_map = self.imgToAnns
                    self.cat_img_map = self.catToImgs

                def get_ann_ids(self, img_ids=[], cat_ids=[], area_rng=[], iscrowd=None):
                    return self.getAnnIds(img_ids, cat_ids, area_rng, iscrowd)

                def get_cat_ids(self, cat_names=[], sup_names=[], cat_ids=[]):
                    return self.getCatIds(cat_names, sup_names, cat_ids)

                def get_img_ids(self, img_ids=[], cat_ids=[]):
                    return self.getImgIds(img_ids, cat_ids)

                def load_anns(self, ids):
                    return self.loadAnns(ids)

                def load_cats(self, ids):
                    return self.loadCats(ids)

                def load_imgs(self, ids):
                    return self.loadImgs(ids)

            coco = COCO(self.ann_2d_file)
            proposal_coco = coco.loadRes(self.proposal_file)
            del coco
            self.proposal_coco = COCO_wrapper(proposal_coco)

        self.cat_ids = self.proposal_coco.get_cat_ids(cat_names=self.CLASSES)
        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}
        self.impath_to_imgid = {}
        self.imgid_to_dataid = {}
        data_infos = []
        total_ann_ids = []
        for i in self.proposal_coco.get_img_ids():
            info = self.proposal_coco.load_imgs([i])[0]
            info['filename'] = info['file_name']
            self.impath_to_imgid['./data/nuscenes/' + info['file_name']] = i
            self.imgid_to_dataid[i] = len(data_infos)
            data_infos.append(info)
            ann_ids = self.proposal_coco.get_ann_ids(img_ids=[i])
            total_ann_ids.extend(ann_ids)
        assert len(set(total_ann_ids)) == len(
            total_ann_ids), f"Annotation ids in '{self.proposal_file}' are not unique!"
        self.proposal_infos = data_infos

    def impath_to_proposals(self, impath):
        img_id = self.impath_to_imgid[impath]
        data_id = self.imgid_to_dataid[img_id]
        ann_ids = self.proposal_coco.get_ann_ids(img_ids=[img_id])
        ann_info = self.proposal_coco.load_anns(ann_ids)
        return self.get_proposals(self.proposal_infos[data_id], ann_info)

    def get_proposals(self, img_info, proposal_info):
        proposal_bboxes = []
        proposal_labels = []
        proposal_scores = []
        for i, ann in enumerate(proposal_info):
            if ann.get('ignore', False):
                continue
            x1, y1, w, h = ann['bbox']
            inter_w = max(0, min(x1 + w, img_info['width']) - max(x1, 0))
            inter_h = max(0, min(y1 + h, img_info['height']) - max(y1, 0))
            if inter_w * inter_h == 0:
                continue
            if ann['area'] <= 0 or w < 1 or h < 1:
                continue
            if ann['category_id'] not in self.cat_ids:
                continue
            bbox = [x1, y1, x1 + w, y1 + h]
            if not ann.get('iscrowd', False):
                proposal_bboxes.append(bbox)
                proposal_labels.append(self.cat2label[ann['category_id']])
                proposal_scores.append(ann['score'])

        if proposal_bboxes:
            proposal_bboxes = np.array(proposal_bboxes, dtype=np.float32)
            proposal_labels = np.array(proposal_labels, dtype=np.int64)
            proposal_scores = np.array(proposal_scores, dtype=np.float32)
        else:
            proposal_bboxes = np.zeros((0, 4), dtype=np.float32)
            proposal_labels = np.array([], dtype=np.int64)
            proposal_scores = np.array([], dtype=np.float32)

        if self.proposal_nms is not None and len(proposal_bboxes) > 0:
            score_thr = self.proposal_nms['score_thr']
            nms_cfg = self.proposal_nms.get('nms')
            class_agnostic = self.proposal_nms.get('class_agnostic', nms_cfg.get('class_agnostic'))
            max_num = self.proposal_nms.get('max_num', -1)

            # score filter
            valid_mask = proposal_scores > score_thr
            inds = valid_mask.nonzero()[0]
            proposal_bboxes, proposal_scores, proposal_labels = proposal_bboxes[inds], proposal_scores[inds], proposal_labels[inds]

            if len(proposal_bboxes) > 0:
                boxes = torch.from_numpy(proposal_bboxes.clone())
                scores = torch.from_numpy(proposal_scores.clone())
                nms_labels = labels = torch.from_numpy(proposal_labels.clone())
                if class_agnostic:
                    nms_labels = torch.zeros_like(labels)
                dets, keep = batched_nms(boxes, scores, nms_labels, nms_cfg)
                if max_num > 0:
                    dets = dets[:max_num]
                    keep = keep[:max_num]
                labels = labels[keep]

                proposal_bboxes = dets[:, :4].numpy()
                proposal_scores = dets[:, 4].numpy()
                proposal_labels = labels.numpy()

        proposals = np.concatenate([proposal_bboxes, proposal_scores[:, None], proposal_labels[:, None]], axis=-1, dtype=np.float32)
        return proposals

    def _set_sequence_group_flag(self):
        """
        Set each sequence to be a different group
        """
        res = []

        curr_sequence = 0
        for idx in range(len(self.data_infos)):
            if idx != 0 and len(self.data_infos[idx]['sweeps']) == 0:
                # Not first frame and # of sweeps is 0 -> new sequence
                curr_sequence += 1
            res.append(curr_sequence)

        self.flag = np.array(res, dtype=np.int64)

        if self.seq_split_num != 1:
            if self.seq_split_num == 'all':
                self.flag = np.array(range(len(self.data_infos)), dtype=np.int64)
            elif self.seq_length <= 0:
                bin_counts = np.bincount(self.flag)
                new_flags = []
                curr_new_flag = 0
                for curr_flag in range(len(bin_counts)):
                    curr_sequence_length = np.array(
                        list(range(0, bin_counts[curr_flag], math.ceil(bin_counts[curr_flag] / self.seq_split_num)))
                        + [bin_counts[curr_flag]])

                    for sub_seq_idx in (curr_sequence_length[1:] - curr_sequence_length[:-1]):
                        for _ in range(sub_seq_idx):
                            new_flags.append(curr_new_flag)
                        curr_new_flag += 1

                assert len(new_flags) == len(self.flag)
                assert len(np.bincount(new_flags)) == len(np.bincount(self.flag)) * self.seq_split_num
                self.flag = np.array(new_flags, dtype=np.int64)
            else:
                bin_counts = np.bincount(self.flag)
                new_flags = []
                curr_new_flag = 0
                for curr_flag in range(len(bin_counts)):
                    curr_sequence_length = np.array(
                        list(range(0, bin_counts[curr_flag], self.seq_length)) + [bin_counts[curr_flag]])

                    for sub_seq_idx in (curr_sequence_length[1:] - curr_sequence_length[:-1]):
                        for _ in range(sub_seq_idx):
                            new_flags.append(curr_new_flag)
                        curr_new_flag += 1

                assert len(new_flags) == len(self.flag)
                self.flag = np.array(new_flags, dtype=np.int64)

    def prepare_train_data(self, index):
        """
        Training data preparation.
        Args:
            index (int): Index for accessing the target data.
        Returns:
            dict: Training data dict of the corresponding index.
        """
        queue = []
        index_list = list(range(index-self.queue_length-self.random_length+1, index))
        random.shuffle(index_list)
        index_list = sorted(index_list[self.random_length:])
        index_list.append(index)
        prev_scene_token = None
        for i in index_list:
            i = max(0, i)
            input_dict = self.get_data_info(i)
            
            if not self.seq_mode: # for sliding window only
                if input_dict['scene_token'] != prev_scene_token:
                    input_dict.update(dict(prev_exists=False))
                    prev_scene_token = input_dict['scene_token']
                else:
                    input_dict.update(dict(prev_exists=True))

            self.pre_pipeline(input_dict)
            example = self.pipeline(input_dict)
            # example["bboxes3d_cams"] = input_dict["ann_info"]["bboxes3d_cams"]
            queue.append(example)

        for k in range(self.num_frame_losses):
            if self.filter_empty_gt and \
                (queue[-k-1] is None or ~(queue[-k-1]['gt_labels_3d']._data != -1).any()):
                return None
        return self.union2one(queue)

    def prepare_test_data(self, index):
        """Prepare data for testing.

        Args:
            index (int): Index for accessing the target data.

        Returns:
            dict: Testing data dict of the corresponding index.
        """
        input_dict = self.get_data_info(index)
        self.pre_pipeline(input_dict)
        example = self.pipeline(input_dict)
        return example
        
    def union2one(self, queue):
        for key in self.collect_keys:
            if key != 'img_metas':
                queue[-1][key] = DC(torch.stack([each[key].data for each in queue]), cpu_only=False, stack=True, pad_dims=None)
            else:
                queue[-1][key] = DC([each[key].data for each in queue], cpu_only=True)
        for key in ['points']:
            if key in queue[-1]:
                queue[-1][key] = DC([each[key].data for each in queue], cpu_only=False)
        for key in ['proposals']:
            if key in queue[-1]:
                queue[-1][key] = DC([each[key].data for each in queue], cpu_only=False)
        if not self.test_mode:
            for key in ['gt_bboxes_3d', 'gt_labels_3d', 'gt_bboxes', 'gt_labels', 'centers2d', 'depths']:
                if key == 'gt_bboxes_3d':
                    queue[-1][key] = DC([each[key].data for each in queue], cpu_only=True)
                else:
                    queue[-1][key] = DC([each[key].data for each in queue], cpu_only=False)

        queue = queue[-1]
        
        # bbox3d = queue["gt_bboxes_3d"].data[0]  # (N, 9)
        # all_views_match = []

        # for cam_id in range(6):
        #     # Lấy ma trận chiếu LiDAR → Image
        #     lidar2img = queue["lidar2img"].data[0][cam_id]  # (4, 4)
        #     print(lidar2img.shape)
        #     # Lấy 8 corners của bbox trong hệ LiDAR
        #     corners_lidar = bbox3d.corners  # (N, 8, 3)
        #     num_bbox, num_pts = corners_lidar.shape[:2]

        #     # Chuyển sang homogeneous coordinate
        #     ones = corners_lidar.new_ones((num_bbox, num_pts, 1))  # (N, 8, 1)
        #     points_hom = torch.cat([corners_lidar, ones], dim=-1)  # (N, 8, 4)

        #     # Project sang ảnh qua lidar2img
        #     points_flat = points_hom.view(-1, 4).T                    # (4, N*8)
        #     proj = (lidar2img @ points_flat).T                        # (N*8, 4)
        #     proj = proj.view(num_bbox, num_pts, 4)                    # (N, 8, 4)

        #     # Chuẩn hóa thành tọa độ ảnh 2D
        #     eps = 1e-5
        #     xy = proj[..., :2] / (proj[..., 2:3] + eps)               # (N, 8, 2)

        #     # Tính bounding box 2D (x1, y1, x2, y2) từ các corners
        #     bbox_2d_projected = torch.cat([
        #         xy.min(dim=1).values,  # (N, 2)
        #         xy.max(dim=1).values   # (N, 2)
        #     ], dim=-1)  # (N, 4)

        #     # Lấy GT bbox 2D tương ứng với camera view này
        #     gt_bboxes_2d = queue["gt_bboxes"].data[0][cam_id]  # (M, 4)

        #     if len(gt_bboxes_2d) == 0:
        #         continue  # Không có GT nào trong view này

        #     # Tính IoU giữa predicted và GT
        #     ious = bbox_overlaps(bbox_2d_projected, gt_bboxes_2d, mode='iou')  # (N, M)
        #     iou_max, gt_idx = ious.max(dim=1)  # (N,)
        #     matched = iou_max >= 0.5

        #     # matched_bbox_pred = bbox_2d_projected[matched] # (K, 4)
        #     bbox3d_matched = bbox3d[matched]
        #     matched_bbox_gt   = gt_bboxes_2d[gt_idx[matched]]         # (K, 4)
            
        #     view_data = {
        #         "cam_idx": cam_id,
        #         "matched_pred": bbox3d_matched,
        #         "matched_gt": matched_bbox_gt,
        #         "iou": iou_max[matched]
        #     }
        #     all_views_match.append(view_data)

        # # Lưu tất cả matched pairs từ các views
        # queue["pairs"] = all_views_match
        
        return queue

    def get_data_info(self, index):
        """Get data info according to the given index.

        Args:
            index (int): Index of the sample data to get.

        Returns:
            dict: Data information that will be passed to the data \
                preprocessing pipelines. It includes the following keys:

                - sample_idx (str): Sample index.
                - pts_filename (str): Filename of point clouds.
                - sweeps (list[dict]): Infos of sweeps.
                - timestamp (float): Sample timestamp.
                - img_filename (str, optional): Image filename.
                - lidar2img (list[np.ndarray], optional): Transformations \
                    from lidar to different cameras.
                - ann_info (dict): Annotation info.
        """
        info = self.data_infos[index]
        # print(info['bboxes3d_cams'][0].shape, info['bboxes3d_cams'][1].shape, info['bboxes3d_cams'][2].shape)
        # standard protocal modified from SECOND.Pytorch

        e2g_rotation = Quaternion(info['ego2global_rotation']).rotation_matrix
        e2g_translation = info['ego2global_translation']
        l2e_rotation = Quaternion(info['lidar2ego_rotation']).rotation_matrix
        l2e_translation = info['lidar2ego_translation']
        e2g_matrix = convert_egopose_to_matrix_numpy(e2g_rotation, e2g_translation)
        l2e_matrix = convert_egopose_to_matrix_numpy(l2e_rotation, l2e_translation)
        ego_pose =  e2g_matrix @ l2e_matrix # lidar2global

        ego_pose_inv = invert_matrix_egopose_numpy(ego_pose)
        input_dict = dict(
            sample_idx=info['token'],
            pts_filename=info['lidar_path'],
            sweeps=info['sweeps'],
            ego_pose=ego_pose,
            ego_pose_inv = ego_pose_inv,
            prev_idx=info['prev'],
            next_idx=info['next'],
            scene_token=info['scene_token'],
            frame_idx=info['frame_idx'],
            timestamp=info['timestamp'] / 1e6,
        )

        if self.modality['use_camera']:
            image_paths = []
            lidar2img_rts = []
            intrinsics = []
            extrinsics = []
            img_timestamp = []
            for cam_type, cam_info in info['cams'].items():
                img_timestamp.append(cam_info['timestamp'] / 1e6)
                image_paths.append(cam_info['data_path'])
                # obtain lidar to image transformation matrix
                cam2lidar_r = cam_info['sensor2lidar_rotation']
                cam2lidar_t = cam_info['sensor2lidar_translation']
                cam2lidar_rt = convert_egopose_to_matrix_numpy(cam2lidar_r, cam2lidar_t)
                lidar2cam_rt = invert_matrix_egopose_numpy(cam2lidar_rt)

                intrinsic = cam_info['cam_intrinsic']
                viewpad = np.eye(4)
                viewpad[:intrinsic.shape[0], :intrinsic.shape[1]] = intrinsic
                lidar2img_rt = (viewpad @ lidar2cam_rt)
                intrinsics.append(viewpad)
                extrinsics.append(lidar2cam_rt)
                lidar2img_rts.append(lidar2img_rt)
                
            if not self.test_mode: # for seq_mode
                prev_exists  = not (index == 0 or self.flag[index - 1] != self.flag[index])
            else:
                prev_exists = None

            input_dict.update(
                dict(
                    img_timestamp=img_timestamp,
                    img_filename=image_paths,
                    lidar2img=lidar2img_rts,
                    intrinsics=intrinsics,
                    extrinsics=extrinsics,
                    prev_exists=prev_exists,
                ))

            if self.proposal_file is not None:
                proposals = []
                for cam_i in range(len(image_paths)):
                    proposals_cam_i = self.impath_to_proposals(image_paths[cam_i])
                    proposals.append(proposals_cam_i)
                input_dict['proposals'] = proposals

        if not self.test_mode:
            annos = self.get_ann_info(index)
            annos.update( 
                dict(
                    bboxes=info['bboxes2d'],
                    labels=info['labels2d'],
                    bboxes3d_cams=info['bboxes3d_cams'],
                    centers2d=info['centers2d'],
                    depths=info['depths'],
                    bboxes_ignore=info['bboxes_ignore'])
            )
            input_dict['ann_info'] = annos
            
        return input_dict


    def __getitem__(self, idx):
        """Get item from infos according to the given index.
        Returns:
            dict: Data dictionary of the corresponding index.
        """
        if self.test_mode:
            return self.prepare_test_data(idx)
        while True:

            data = self.prepare_train_data(idx)
            if data is None:
                idx = self._rand_another(idx)
                continue
            return data

    def _evaluate_single(self,
                         result_path,
                         logger=None,
                         metric='bbox',
                         result_name='pts_bbox'):
        """Evaluation for a single model in nuScenes protocol.

        Args:
            result_path (str): Path of the result file.
            logger (logging.Logger | str, optional): Logger used for printing
                related information during evaluation. Default: None.
            metric (str, optional): Metric name used for evaluation.
                Default: 'bbox'.
            result_name (str, optional): Result name in the metric prefix.
                Default: 'pts_bbox'.

        Returns:
            dict: Dictionary of evaluation details.
        """
        from nuscenes import NuScenes
        from nuscenes.eval.detection.evaluate import NuScenesEval

        output_dir = osp.join(*osp.split(result_path)[:-1])
        nusc = NuScenes(
            version=self.version, dataroot=self.data_root, verbose=False)
        eval_set_map = {
            'v1.0-mini': 'mini_val',
            'v1.0-trainval': 'val',
            # 'v1.0-trainval': 'train',
        }
        nusc_eval = NuScenesEval(
            nusc,
            config=self.eval_detection_configs,
            result_path=result_path,
            eval_set=eval_set_map[self.version],
            output_dir=output_dir,
            verbose=False)

        nusc_eval.main(render_curves=False)

        # record metrics
        metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))
        detail = dict()
        metric_prefix = f'{result_name}_NuScenes'
        for name in self.CLASSES:
            for k, v in metrics['label_aps'][name].items():
                val = float('{:.4f}'.format(v))
                detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val
            for k, v in metrics['label_tp_errors'][name].items():
                val = float('{:.4f}'.format(v))
                detail['{}/{}_{}'.format(metric_prefix, name, k)] = val
            for k, v in metrics['tp_errors'].items():
                val = float('{:.4f}'.format(v))
                detail['{}/{}'.format(metric_prefix,
                                      self.ErrNameMapping[k])] = val

        detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']
        detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']
        return detail


def invert_matrix_egopose_numpy(egopose):
    """ Compute the inverse transformation of a 4x4 egopose numpy matrix."""
    inverse_matrix = np.zeros((4, 4), dtype=np.float32)
    rotation = egopose[:3, :3]
    translation = egopose[:3, 3]
    inverse_matrix[:3, :3] = rotation.T
    inverse_matrix[:3, 3] = -np.dot(rotation.T, translation)
    inverse_matrix[3, 3] = 1.0
    return inverse_matrix

def convert_egopose_to_matrix_numpy(rotation, translation):
    transformation_matrix = np.zeros((4, 4), dtype=np.float32)
    transformation_matrix[:3, :3] = rotation
    transformation_matrix[:3, 3] = translation
    transformation_matrix[3, 3] = 1.0
    return transformation_matrix
